##Todo:##
1. The action is essentially binary
-- We can try discretize the action space into trinary

2. Fix a joint, train the other joint
-- alternatively, Train togather first, and then fix one move another

3. Why is the Reynold training much slower? 
-- think about the sequence of devision

4. Try action baseline on Reynolds

5. Try changing the baseline 
	**+ sin + cos
	change the phase => 90%

6. Think about how to seperate the experience

7. Guided policy search?

8. Cyclic => some terminal state vs some punishment for not moving
	- not moving forward => result in terminal
	- force a gait usage (use a action from a previous phase)
	**- Sampling w. baseline

9. Model-based
	- 


##Thoughts:##
1. Enforce cyclic motion
	Option 1: I found it very hard to phrase it:
					- discard the action if it is not its turn
					- action include freq + amp
	Option 2: the CMU paper
	-- For the phase:
			q1 - q2?
			Then a delta q that signifies the change of this
	Option 3: cpg
	Option 4: baseline action

2. How to make sure that the start and end poses are the same?
-- Just append an adjusting parameter at the end



##Things to try later:##
1. Hierachical RL:
- Having multi-baseline
- Train based on that
- sample baseline
- Update the baseline through evolution

2. Model based method could work
V(q1, q2, theta)
Transition

3. Learning the bounding box (what should be the start and end phase of the robot?)
-- Have this working With different terrains

reynolds: step time: 0.03s
baseline: step time: 0.001s