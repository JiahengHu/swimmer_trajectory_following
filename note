robot = get_robot(args.robot)(xmlfile)
params = robot.params()

use normed parameters

env is just normal environment (which we can replace with our own environment)
The robot parames are passed in through the xml file (which we can change to make it hardcoded)

Everytime, observation is concatenated with the robot parameters in the augmented environment

Could be the initialization or the assignment problem

right now I set the range to 0.1, 0.1, 0.3
While the sampling is around these values
(0.05 - 0.2) (0.05 - 0.2) (0.15 - 0.6)


It is not using the GPU... So it might be using CPU / stuck at somewhere

1. It is on-policy, meaning that a replay buffer is not being used
2. rollout_length = 1024??? Meaning it will collect 1024 amount of data and then use it to do update?


Also, need a way to save the robot parameters

episode length: need to change in both env and init

change the save freq
change the number of episode


chop means halve, this use the mpi_robot, and currently haven't been tested at all (so we might actually get some bug when dealing with this)

self.timesteps_per_step = self.nenv * self.rollout_length
							8*64 (previously 1024)
							How many actions in total
We have 8 envs in total everytime

8 processes, only number 1 env saves the result

all with rsp to t: initially 1e9

if rollout = 1024*8
	doesn't matter

10*8 (pre) + 8*10*8 (update) + 10*8 (post)
	Looks like something wrong? Cos we really only need 3 chops

	chop set the probability to 0, and the probability is synchronized across the environments, so the number of enviornment has nothing to do with the number of prob distri

512 + 2*512 + 512

chopping takes a long time (but it is probably reasonable)
Why does it chop on round 4 (instead of round 1, 2, 3?)

choping is taking too much time
#experiment: set sampling to 1, see how it goes


ctr A ctr D

screen -r
screen -S

hanabi_no_seed_ben_gpu2

aux



ssh -p 23 biorobotics@highbay.pc.cs.cmu.edu


What happens to the constraint??? 

____________________________________________________________________________________________

The start of gait optimization
1. If we stick with PPO, we need to make sure that 

Note:
1. For now, in right_hand_side, I removed the T[0]. Add it back if necessary
2. Set up the q1 and q2 amplitude & frequency

3. What should be the time interval? For now I set it to 1 / 20

4. Think about the length of the training: how can we make it so that it will learn to collect useful data? [We can end episode if the robot is too far away from the target!! [and get a negetive reward? Maybe just zero reward]]

5. The number of points to return: currently 10, should we make it bigger / smaller?

6. reward scaling & distance of path scaling

7. stop episode num

8. Add the previous action

cleanup: change triple reset to reset
