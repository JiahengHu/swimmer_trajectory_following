robot = get_robot(args.robot)(xmlfile)
params = robot.params()

use normed parameters

env is just normal environment (which we can replace with our own environment)
The robot parames are passed in through the xml file (which we can change to make it hardcoded)

Everytime, observation is concatenated with the robot parameters in the augmented environment

Could be the initialization or the assignment problem

right now I set the range to 0.1, 0.1, 0.3
While the sampling is around these values
(0.05 - 0.2) (0.05 - 0.2) (0.15 - 0.6)


It is not using the GPU... So it might be using CPU / stuck at somewhere

1. It is on-policy, meaning that a replay buffer is not being used
2. rollout_length = 1024??? Meaning it will collect 1024 amount of data and then use it to do update?


Also, need a way to save the robot parameters

episode length: need to change in both env and init

change the save freq
change the number of episode


chop means halve, this use the mpi_robot, and currently haven't been tested at all (so we might actually get some bug when dealing with this)

self.timesteps_per_step = self.nenv * self.rollout_length
							8*64 (previously 1024)
							How many actions in total
We have 8 envs in total everytime

8 processes, only number 1 env saves the result

all with rsp to t: initially 1e9

if rollout = 1024*8
	doesn't matter

10*8 (pre) + 8*10*8 (update) + 10*8 (post)
	Looks like something wrong? Cos we really only need 3 chops

	chop set the probability to 0, and the probability is synchronized across the environments, so the number of enviornment has nothing to do with the number of prob distri

512 + 2*512 + 512

chopping takes a long time (but it is probably reasonable)
Why does it chop on round 4 (instead of round 1, 2, 3?)

choping is taking too much time
#experiment: set sampling to 1, see how it goes


ctr A ctr D

screen -r
screen -S

hanabi_no_seed_ben_gpu2

aux



ssh -p 23 biorobotics@highbay.pc.cs.cmu.edu


What happens to the constraint??? 

____________________________________________________________________________________________

The start of gait optimization
1. If we stick with PPO, we need to make sure that 


only works for one env, otherwise memory out

right now it is way slower than their env (per step), even though the batch size is significantly smaller, why? Is it really that different? (1024 steps is significantly faster than 64 steps?)

growth == true might hamper the speed?

result for 1e6:
robot/mass_1 value: 0.0795760452747345
robot/mass_2 value: 0.13437068462371826
robot/mass_3 value: 0.06639838814735413
robot/k_constant_1 value: 0.10338518023490907
robot/k_constant_2 value: 0.08443064689636232
robot/link_length_1 value: 0.408584064245224
robot/link_length_2 value: 0.2789143770933151
robot/link_length_3 value: 0.18489774763584135




50s:
getting called 1585 times
replace dummy symbols 0.0006654262542724609 seconds ---
solve to get a matrix solution 1.9550323486328125e-05 seconds ---
odeint 1.2791054248809814 seconds ---
0.00002

5s:
getting called 203 times
replace dummy symbols 0.0006403923034667969 seconds ---
solve to get a matrix solution 1.9073486328125e-05 seconds ---
odeint 0.20050811767578125 seconds ---


simplify doubles the speed
msub?
doesn't improve speed

if substituting in the param doens't work, probably has to resort to c solution
currently: kinda worked

-----------------------------------------------------------

Note:
1. For now, in right_hand_side, I removed the T[0]. Add it back if necessary
2. Set up the q1 and q2 amplitude & frequency

3. What should be the time interval? For now I set it to 1 / 20

4. Think about the length of the training: how can we make it so that it will learn to collect useful data? [We can end episode if the robot is too far away from the target!! [and get a negetive reward? Maybe just zero reward]]

5. The number of points to return: currently 10, should we make it bigger / smaller?

6. reward scaling & distance of path scaling

7. stop episode num

8. Add the previous action

cleanup: change triple reset to reset

9. Look up how the range is getting used

Problem:
I think the action space is confusing the robot too much that it don't know what to do
1. We can discretize the action space
2. We can try to change the reward structure
	- Change it so that we encourage moving?

The trajectory also looks weird: why is it straigh lines?

The reward scale needs to be changed: the robot gets a lot of reward by going back and forth at a local scale

!!because the robot is too far away from the path
We need to at least adjust the gaussian threshold
# because we can't stay close to the track at every single timestamp